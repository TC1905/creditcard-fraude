
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split

#import
data = pd.read_csv(r'C:\Users\Gebruiker\Desktop\creditcard.csv')
# check for nu values
data.isnull().sum()
# drop duplicated rows
data = data.drop_duplicates()
# normalize
sc = RobustScaler()
data['Time'] = sc.fit_transform(data['Time'].values.reshape(-1,1))
data['Amount'] = sc.fit_transform(data['Amount'].values.reshape(-1,1))
#checking for imbalances 
data['Class'].value_counts()

# lets make a chart to see the data visualy
data['Class'].value_counts().plot(kind='bar', figsize=(12, 5))
print('class_0_% is: {}'.format(100*(data['Class'].value_counts()[0]/len(data))))
print('class_0_% is: {}'.format(100*(data['Class'].value_counts()[1]/len(data))))
# we see that the data is unbalance
# we are going to use algorithe 

(train, test) = train_test_split(data, test_size=0.2, stratify = data['Class'])
train

train['Class'].value_counts().plot(kind='bar', figsize=(12, 5))
print('class_0_% is: {}'.format(100*(train['Class'].value_counts()[0]/len(train))))
print('class_0_% is: {}'.format(100*(train['Class'].value_counts()[1]/len(train))))
test

test['Class'].value_counts().plot(kind='bar', figsize=(12, 5))
print('class_0_% is: {}'.format(100*(test['Class'].value_counts()[0]/len(test))))
print('class_0_% is: {}'.format(100*(test['Class'].value_counts()[1]/len(test))))


train_X = train.iloc[:,:-1].values
train_y = train.iloc[:, -1].values

X_test = test.iloc[:,:-1].values
y_test = test.iloc[:,-1].values

from sklearn.linear_model import LogisticRegression
before_lr = LogisticRegression()
before_lr.fit(train_X, train_y)

before_y_pred_lr = before_lr.predict(X_test)

from sklearn.neighbors import KNeighborsClassifier
before_knn = KNeighborsClassifier(n_neighbors = 5)
before_knn.fit(train_X, train_y)

before_y_pred_knn = before_knn.predict(X_test)


from imblearn.over_sampling import SMOTE
X_resampled, y_resampled = SMOTE().fit_resample(train_X, train_y)

X_resampled

## Now, visualize the distribution after applying SMOTE
categories = [0,1]
values = [len(y_resampled[y_resampled == 0]), len(y_resampled[y_resampled == 1])]
plt.bar(categories, values)
plt.title('Class_Distribution after applying SMOTE')
plt.show()

# training the model with logisticregression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_resampled, y_resampled)

y_pred_lr = lr.predict(test.iloc[:,:-1].values)
# training model with knn
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_resampled, y_resampled)

y_pred_knn = knn.predict(X_test)


from sklearn.metrics import confusion_matrix, accuracy_score
tn, fp, fn, tp = confusion_matrix(y_test, before_y_pred_lr).ravel()
print('Before SMOTE, for LogisticRegression')
print('True Negatives: {}\nFalse Positives: {}\nFalse Negatives: {}\nTrue Positives: {}\nAccuracy_Score: {}'.format(tn, fp, fn, tp, 100* accuracy_score(y_test, before_y_pred_lr)))
tn, fp, fn, tp = confusion_matrix(y_test, before_y_pred_knn).ravel()
print('Before SMOTE, for KNN')
print('True Negatives: {}\nFalse Positives: {}\nFalse Negatives: {}\nTrue Positives: {}\nAccuracy_Score: {}'.format(tn, fp, fn, tp, 100* accuracy_score(y_test, before_y_pred_knn)))
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()
print('After SMOTE, for LogisticRegression')
print('True Negatives: {}\nFalse Positives: {}\nFalse Negatives: {}\nTrue Positives: {}\nAccuracy_Score: {}'.format(tn, fp, fn, tp, 100* accuracy_score(y_test, y_pred_lr)))                                  
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()
print('After SMOTE, for KNN ')
print('True Negatives: {}\nFalse Positives: {}\nFalse Negatives: {}\nTrue Positives: {}\nAccuracy_Score: {}'.format(tn, fp, fn, tp, 100* accuracy_score(y_test, y_pred_knn)))

from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, y_pred_knn)
